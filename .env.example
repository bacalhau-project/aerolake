# =============================================================================
# DATABRICKS WIND TURBINE PIPELINE ENVIRONMENT CONFIGURATION
# =============================================================================
# This file contains all environment variables needed for the pipeline.
# Copy this to .env and fill in your actual values.

# =============================================================================
# AWS CONFIGURATION
# =============================================================================
AWS_REGION=us-west-2
AWS_ACCOUNT_ID=123456789012
AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
AWS_DEFAULT_REGION=${AWS_REGION}

# =============================================================================
# S3 BUCKET CONFIGURATION
# =============================================================================
S3_BUCKET_PREFIX=your-company-name
S3_REGION=${AWS_REGION}

# Derived S3 bucket names (auto-generated from prefix)
S3_BUCKET_RAW=${S3_BUCKET_PREFIX}-raw-data-${S3_REGION}
S3_BUCKET_VALIDATED=${S3_BUCKET_PREFIX}-validated-data-${S3_REGION}
S3_BUCKET_ANOMALIES=${S3_BUCKET_PREFIX}-anomalies-${S3_REGION}
S3_BUCKET_SCHEMATIZED=${S3_BUCKET_PREFIX}-schematized-data-${S3_REGION}
S3_BUCKET_AGGREGATED=${S3_BUCKET_PREFIX}-aggregated-data-${S3_REGION}
S3_BUCKET_CHECKPOINTS=${S3_BUCKET_PREFIX}-checkpoints-${S3_REGION}
S3_BUCKET_METADATA=${S3_BUCKET_PREFIX}-metadata-${S3_REGION}

# =============================================================================
# DATABRICKS CONFIGURATION
# =============================================================================
DATABRICKS_HOST=your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapi-abcdef1234567890abcdef1234567890
DATABRICKS_HTTP_PATH=/sql/1.0/endpoints/abcdef1234567890
DATABRICKS_DATABASE=wind_turbine_sensors
DATABRICKS_CATALOG=main
DATABRICKS_SCHEMA=sensor_data
DATABRICKS_ACCOUNT_ID=414351767826

# Databricks SQL Warehouse/Cluster settings
DATABRICKS_CLUSTER_ID=your-cluster-id
DATABRICKS_WAREHOUSE_ID=your-warehouse-id

# =============================================================================
# BACALHAU CONFIGURATION
# =============================================================================
BACALHAU_API_HOST=localhost
BACALHAU_API_PORT=1234
BACALHAU_NODE_ID=your-bacalhau-node-id

# Container Registry for Bacalhau jobs
GITHUB_TOKEN=ghp_1234567890abcdef1234567890abcdef123456
GITHUB_USER=your-github-username
REGISTRY=ghcr.io
ORGANIZATION_NAME=your-org
IMAGE_NAME=databricks-uploader

# =============================================================================
# PIPELINE CONFIGURATION
# =============================================================================
# SQLite Source Database
SQLITE_PATH=/opt/sensor/data/sensor_data.db
SQLITE_DB_PATH=${SQLITE_PATH}
SQLITE_TABLE_NAME=sensor_readings
TIMESTAMP_COL=timestamp

# Processing Settings
BATCH_SIZE=500
UPLOAD_INTERVAL=15
MAX_BATCH_SIZE_MB=50
PROCESSING_THREADS=4

# State Management
STATE_DIR=/bacalhau_data/state
UPLOAD_STATE_FILE=${STATE_DIR}/s3-uploader/upload_state.json
PIPELINE_CONFIG_DB=${STATE_DIR}/pipeline_config.db

# =============================================================================
# MONITORING & NOTIFICATIONS
# =============================================================================
# SQS Configuration (for anomaly notifications)
SQS_QUEUE_NAME=wind-turbine-anomaly-notifications
SQS_REGION=${AWS_REGION}
SQS_QUEUE_URL=https://sqs.${SQS_REGION}.amazonaws.com/${AWS_ACCOUNT_ID}/${SQS_QUEUE_NAME}

# Monitoring Dashboard
MONITORING_PORT=8000
API_HOST=0.0.0.0
LOG_LEVEL=INFO

# =============================================================================
# SCHEMA VALIDATION
# =============================================================================
SCHEMA_URL=https://raw.githubusercontent.com/your-org/wind-turbine-schema/main/schema.json
SCHEMA_LOCAL_PATH=./wind-turbine-schema.json
ENABLE_SCHEMA_VALIDATION=true
ENABLE_AUTO_DETECTION=true

# =============================================================================
# SECURITY & CREDENTIALS
# =============================================================================
# AWS Credentials Directory (for container mounts)
AWS_CREDENTIALS_DIR=/bacalhau_data/credentials
CREDENTIALS_FILE=${AWS_CREDENTIALS_DIR}/expanso-s3-env.sh

# SSL/TLS Settings
SSL_VERIFY=true
DATABRICKS_SSL_VERIFY=true

# =============================================================================
# DEVELOPMENT & DEBUGGING
# =============================================================================
DEBUG=false
VERBOSE=false
DRY_RUN=false
ENABLE_PROFILING=false

# Test Environment Settings
TEST_MODE=false
MOCK_S3_UPLOADS=false
MOCK_DATABRICKS=false