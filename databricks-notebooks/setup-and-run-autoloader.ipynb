{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c67e25-bb90-423a-b0de-992e65b6105e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Upload Metadata\n",
    "- **Uploaded:** 2025-08-18 08:48:21\n",
    "- **Validated:** ✓ (syntax check passed)\n",
    "- **User:** aronchick@gmail.com\n",
    "- **Source:** setup-and-run-autoloader.py\n",
    "\n",
    "---\n",
    "\n",
    "%md\n",
    "# Simple Auto Loader Polling Pipeline\n",
    "\n",
    "This notebook runs 5 independent Auto Loader pipelines that continuously ingest data from S3 buckets to Unity Catalog tables.\n",
    "\n",
    "**Architecture:**\n",
    "- 5 S3 buckets (ingestion, validated, anomalies, enriched, aggregated) receive data from external service\n",
    "- 5 Auto Loader pipelines read from these buckets\n",
    "- Data is written to 5 corresponding Unity Catalog tables\n",
    "- Pipelines restart every 30 seconds to process new files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7383df9e-977a-4a25-b9cc-6242c989e43e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032aac43-2d53-4d02-9a4b-fd40853c4166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n\uD83D\uDCCA Tables: ingestion, validated, anomalies, enriched, aggregated\n⏱️  Poll every 30 seconds for 60 minutes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "import random\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Catalog and Schema\n",
    "CATALOG = \"expanso_databricks_workspace\"\n",
    "SCHEMA = \"sensor_readings\"\n",
    "\n",
    "# S3 Buckets (source)\n",
    "BUCKETS = {\n",
    "    \"ingestion\": \"s3://expanso-raw-data-us-west-2/\",\n",
    "    \"validated\": \"s3://expanso-validated-data-us-west-2/\",\n",
    "    \"anomalies\": \"s3://expanso-anomalies-data-us-west-2/\",\n",
    "    \"enriched\": \"s3://expanso-schematized-data-us-west-2/\",\n",
    "    \"aggregated\": \"s3://expanso-aggregated-data-us-west-2/\",\n",
    "}\n",
    "\n",
    "# Unity Catalog Tables (destination)\n",
    "TABLES = {\n",
    "    \"ingestion\": f\"{CATALOG}.{SCHEMA}.sensor_readings_ingestion\",\n",
    "    \"validated\": f\"{CATALOG}.{SCHEMA}.sensor_readings_validated\",\n",
    "    \"anomalies\": f\"{CATALOG}.{SCHEMA}.sensor_readings_anomalies\",\n",
    "    \"enriched\": f\"{CATALOG}.{SCHEMA}.sensor_readings_enriched\",\n",
    "    \"aggregated\": f\"{CATALOG}.{SCHEMA}.sensor_readings_aggregated\",\n",
    "}\n",
    "\n",
    "# Checkpoint and Schema Locations\n",
    "CHECKPOINT_BASE = \"s3://expanso-checkpoints-us-west-2\"\n",
    "SCHEMA_BASE = \"s3://expanso-metadata-us-west-2/schemas\"\n",
    "\n",
    "# Polling Configuration\n",
    "POLL_INTERVAL_SECONDS = 30  # How often to restart pipelines\n",
    "RUNTIME_MINUTES = 60  # Total runtime\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"\uD83D\uDCCA Tables: {', '.join(TABLES.keys())}\")\n",
    "print(f\"⏱️  Poll every {POLL_INTERVAL_SECONDS} seconds for {RUNTIME_MINUTES} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d2087c-460c-4c79-97c2-7373d90655a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Bucket Access & Clear Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f891b0-61bf-4c77-ad5f-059d5e4119eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Testing S3 Bucket Access\n========================================\n✅ ingestion    : 25415 JSON files found\n✅ validated    : 9886 JSON files found\n✅ anomalies    : 1 JSON files found\n✅ enriched     : 7 JSON files found\n✅ aggregated   : 234 JSON files found\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_bucket_access():\n",
    "    \"\"\"Test read access to all S3 buckets.\"\"\"\n",
    "    print(\"\uD83D\uDD0D Testing S3 Bucket Access\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    for stage, bucket in BUCKETS.items():\n",
    "        try:\n",
    "            files = dbutils.fs.ls(bucket)\n",
    "            json_files = [f for f in files if f.name.endswith(\".json\")]\n",
    "            print(f\"✅ {stage:12} : {len(json_files)} JSON files found\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {stage:12} : {str(e)[:50]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def upload_schema_samples():\n",
    "    \"\"\"Upload sample schema files to each bucket for Auto Loader schema inference.\"\"\"\n",
    "    print(\"\uD83D\uDCE4 Uploading Schema Sample Files\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "\n",
    "    # Sample schemas for each stage\n",
    "    samples = {\n",
    "        \"ingestion\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sensor_id\": \"SCHEMA_SAMPLE\",\n",
    "                \"temperature\": 22.0,\n",
    "                \"humidity\": 60.0,\n",
    "                \"pressure\": 101325.0,\n",
    "                \"vibration\": 0.5,\n",
    "                \"voltage\": 24.0,\n",
    "                \"status_code\": 0,\n",
    "                \"anomaly_flag\": 0,\n",
    "                \"firmware_version\": \"1.0.0\",\n",
    "                \"model\": \"SampleModel\",\n",
    "                \"manufacturer\": \"SampleMfg\",\n",
    "                \"location\": \"Sample Location\",\n",
    "                \"latitude\": 0.0,\n",
    "                \"longitude\": 0.0,\n",
    "                \"original_timezone\": \"+00:00\",\n",
    "                \"synced\": 0,\n",
    "            }\n",
    "        ],\n",
    "        \"validated\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sensor_id\": \"SCHEMA_SAMPLE\",\n",
    "                \"temperature\": 22.0,\n",
    "                \"humidity\": 60.0,\n",
    "                \"pressure\": 101325.0,\n",
    "                \"vibration\": 0.5,\n",
    "                \"voltage\": 24.0,\n",
    "                \"status_code\": 0,\n",
    "                \"anomaly_flag\": 0,\n",
    "                \"anomaly_type\": None,\n",
    "                \"firmware_version\": \"1.0.0\",\n",
    "                \"model\": \"SampleModel\",\n",
    "                \"manufacturer\": \"SampleMfg\",\n",
    "                \"location\": \"Sample Location\",\n",
    "                \"latitude\": 0.0,\n",
    "                \"longitude\": 0.0,\n",
    "                \"original_timezone\": \"+00:00\",\n",
    "                \"synced\": 0,\n",
    "                \"validation_status\": \"valid\",\n",
    "            }\n",
    "        ],\n",
    "        \"enriched\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sensor_id\": \"SCHEMA_SAMPLE\",\n",
    "                \"temperature\": 22.0,\n",
    "                \"humidity\": 60.0,\n",
    "                \"pressure\": 101325.0,\n",
    "                \"vibration\": 0.5,\n",
    "                \"voltage\": 24.0,\n",
    "                \"status_code\": 0,\n",
    "                \"anomaly_flag\": 0,\n",
    "                \"firmware_version\": \"1.0.0\",\n",
    "                \"model\": \"SampleModel\",\n",
    "                \"manufacturer\": \"SampleMfg\",\n",
    "                \"location\": \"Sample Location\",\n",
    "                \"latitude\": 0.0,\n",
    "                \"longitude\": 0.0,\n",
    "                \"original_timezone\": \"+00:00\",\n",
    "                \"synced\": 0,\n",
    "                \"enrichment_timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "        ],\n",
    "        \"aggregated\": [\n",
    "            {\n",
    "                \"window_start\": datetime.now().isoformat(),\n",
    "                \"window_end\": datetime.now().isoformat(),\n",
    "                \"sensor_id\": \"SCHEMA_SAMPLE\",\n",
    "                \"avg_temperature\": 22.0,\n",
    "                \"min_temperature\": 20.0,\n",
    "                \"max_temperature\": 24.0,\n",
    "                \"avg_humidity\": 60.0,\n",
    "                \"avg_pressure\": 101325.0,\n",
    "                \"avg_vibration\": 0.5,\n",
    "                \"avg_voltage\": 24.0,\n",
    "                \"record_count\": 300,\n",
    "                \"anomaly_count\": 0,\n",
    "            }\n",
    "        ],\n",
    "        \"anomalies\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sensor_id\": \"SCHEMA_SAMPLE\",\n",
    "                \"temperature\": 45.0,\n",
    "                \"humidity\": 60.0,\n",
    "                \"pressure\": 101325.0,\n",
    "                \"vibration\": 2.5,\n",
    "                \"voltage\": 24.0,\n",
    "                \"status_code\": 1,\n",
    "                \"anomaly_flag\": 1,\n",
    "                \"anomaly_type\": \"spike\",\n",
    "                \"anomaly_score\": 0.95,\n",
    "                \"firmware_version\": \"1.0.0\",\n",
    "                \"model\": \"SampleModel\",\n",
    "                \"manufacturer\": \"SampleMfg\",\n",
    "                \"location\": \"Sample Location\",\n",
    "                \"latitude\": 0.0,\n",
    "                \"longitude\": 0.0,\n",
    "                \"original_timezone\": \"+00:00\",\n",
    "                \"synced\": 0,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    for stage, bucket in BUCKETS.items():\n",
    "        try:\n",
    "            sample_data = samples.get(stage, samples[\"ingestion\"])\n",
    "            json_content = json.dumps(sample_data, indent=2)\n",
    "\n",
    "            # Write sample file to bucket\n",
    "            file_path = f\"{bucket}schema_sample_{timestamp}.json\"\n",
    "            dbutils.fs.put(file_path, json_content, overwrite=True)\n",
    "\n",
    "            print(f\"✅ {stage:12} : Uploaded schema sample\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {stage:12} : Failed to upload sample: {str(e)[:50]}\")\n",
    "\n",
    "    print()\n",
    "    print(\"\uD83D\uDCDD Schema samples uploaded for Auto Loader initialization\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def clear_all_buckets():\n",
    "    \"\"\"Clear all JSON files from S3 buckets and upload schema samples.\"\"\"\n",
    "    print(\"⚠️  CLEARING ALL JSON FILES FROM S3 BUCKETS\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    for stage, bucket in BUCKETS.items():\n",
    "        try:\n",
    "            files = dbutils.fs.ls(bucket)\n",
    "            json_files = [f for f in files if f.name.endswith(\".json\")]\n",
    "\n",
    "            if json_files:\n",
    "                for file in json_files:\n",
    "                    dbutils.fs.rm(file.path)\n",
    "                print(f\"✅ {stage:12} : Cleared {len(json_files)} JSON files\")\n",
    "            else:\n",
    "                print(f\"⚠️  {stage:12} : No JSON files to clear\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {stage:12} : {str(e)[:50]}\")\n",
    "    print()\n",
    "\n",
    "    # Upload schema samples after clearing\n",
    "    upload_schema_samples()\n",
    "\n",
    "\n",
    "# Always test access\n",
    "test_bucket_access()\n",
    "\n",
    "# Uncomment to clear all buckets and initialize with schema samples\n",
    "# clear_all_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0b3a32-1bfb-4cb9-a771-6e7848b39f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d50f5a4-bb38-4a54-b4d8-ac21eb3a7386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_pipeline(stage_name):\n",
    "    \"\"\"\n",
    "    Create an Auto Loader pipeline for a specific stage.\n",
    "\n",
    "    Args:\n",
    "        stage_name: One of 'ingestion', 'validated', 'anomalies', 'enriched', 'aggregated'\n",
    "\n",
    "    Returns:\n",
    "        Streaming query object\n",
    "    \"\"\"\n",
    "    bucket = BUCKETS[stage_name]\n",
    "    table = TABLES[stage_name]\n",
    "\n",
    "    # Read from S3 with Auto Loader\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{SCHEMA_BASE}/{stage_name}\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"cloudFiles.maxFilesPerTrigger\", 100)\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(bucket)\n",
    "    )\n",
    "\n",
    "    # Add processing timestamp\n",
    "    df = df.withColumn(\"processing_timestamp\", F.current_timestamp())\n",
    "\n",
    "    # Write to Unity Catalog table\n",
    "    query = (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/{stage_name}/checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)  # Process available data then stop\n",
    "        .table(table)\n",
    "    )\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed6ec0a-b5bb-42b4-8b02-58b3336d76c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41633970-ecb6-413f-87e3-064320ac8c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_table_count(table_name):\n",
    "    \"\"\"Get current row count for a table.\"\"\"\n",
    "    try:\n",
    "        return spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").first()[0]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def clear_checkpoints():\n",
    "    \"\"\"Clear all checkpoints to start fresh.\"\"\"\n",
    "    for stage in TABLES.keys():\n",
    "        checkpoint_path = f\"{CHECKPOINT_BASE}/{stage}/checkpoint/\"\n",
    "        try:\n",
    "            dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "            print(f\"✅ Cleared {stage} checkpoint\")\n",
    "        except:\n",
    "            print(f\"⚠️  No checkpoint found for {stage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc173e94-c000-4485-b998-0fb09c65f031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clear Checkpoints (Run if needed to reprocess files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30ac7eb-fdc4-47d9-8976-9d9d98d28223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to clear checkpoints and reprocess all files\n",
    "# clear_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0c8216-091a-4994-b7e7-f4597f761dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Polling Pipeline Runner\n",
    "\n",
    "This is the main loop that runs all pipelines on a schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d489f25c-a1f0-4467-b43f-5461eedfa571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 STARTING AUTO LOADER POLLING (PARALLEL)\n============================================================\n\n\uD83D\uDCCA Initial row counts:\n  ingestion    : 1,098,754\n  validated    : 356,942\n  anomalies    : 1\n  enriched     : 2,004\n  aggregated   : 14,662\n\n============================================================\n\uD83D\uDD04 Iteration #1 - 17:00:31 (59 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 123 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,098,877 total (+123 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #2 - 17:01:11 (59 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 93 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,098,970 total (+216 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 31.9 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #3 - 17:01:54 (58 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,062 total (+308 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.6 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #4 - 17:02:32 (57 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 61 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,123 total (+369 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 32.9 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #5 - 17:03:17 (57 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,215 total (+461 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 29.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #6 - 17:03:56 (56 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 93 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,308 total (+554 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.9 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #7 - 17:04:35 (55 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 61 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,369 total (+615 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 29.5 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #8 - 17:05:16 (55 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,461 total (+707 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 30.8 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #9 - 17:05:58 (54 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 93 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,554 total (+800 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 30.8 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #10 - 17:06:40 (53 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 61 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,615 total (+861 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 30.5 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #11 - 17:07:22 (53 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,707 total (+953 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 29.8 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #12 - 17:08:03 (52 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,799 total (+1,045 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 31.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #13 - 17:08:45 (51 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,891 total (+1,137 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 31.7 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #14 - 17:09:28 (51 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 62 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,099,953 total (+1,199 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.2 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #15 - 17:10:07 (50 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,045 total (+1,291 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 29.0 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #16 - 17:10:48 (49 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 62 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,107 total (+1,353 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 32.2 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #17 - 17:11:32 (48 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,199 total (+1,445 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.9 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #18 - 17:12:12 (48 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,291 total (+1,537 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 29.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #19 - 17:12:50 (47 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 62 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,353 total (+1,599 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #20 - 17:13:29 (47 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,445 total (+1,691 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 30.3 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #21 - 17:14:10 (46 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,537 total (+1,783 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 31.3 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #22 - 17:14:53 (45 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 62 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,599 total (+1,845 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #23 - 17:15:31 (45 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,691 total (+1,937 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 32.1 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #24 - 17:16:14 (44 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,783 total (+2,029 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 27.6 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #25 - 17:16:53 (43 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 62 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,845 total (+2,091 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 27.7 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #26 - 17:17:32 (42 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,937 total (+2,183 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 27.7 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #27 - 17:18:12 (42 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 61 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,100,998 total (+2,244 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 32.0 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #28 - 17:18:55 (41 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 92 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,101,090 total (+2,336 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 28.3 seconds (base: 30s ±10%)...\n\n============================================================\n\uD83D\uDD04 Iteration #29 - 17:19:33 (40 min remaining)\n------------------------------------------------------------\n  Starting pipelines in parallel with fuzzing...\n    Submitted ingestion    (delayed ~0.0s ±10%)\n    Submitted validated    (delayed ~0.5s ±10%)\n    Submitted anomalies    (delayed ~1.0s ±10%)\n    Submitted enriched     (delayed ~1.5s ±10%)\n    Submitted aggregated   (delayed ~2.0s ±10%)\n\n  Waiting for pipelines to complete...\n\n  Pipeline Results:\n    aggregated   : ✅ Processed 0 rows\n    anomalies    : ✅ Processed 0 rows\n    enriched     : ✅ Processed 0 rows\n    ingestion    : ✅ Processed 93 rows\n    validated    : ✅ Processed 0 rows\n\n\uD83D\uDCCA Current row counts:\n  ingestion    : 1,101,183 total (+2,429 new)\n  validated    : 356,942 total (+0 new)\n  anomalies    : 1 total (+0 new)\n  enriched     : 2,004 total (+0 new)\n  aggregated   : 14,662 total (+0 new)\n\n\uD83D\uDCA4 Waiting 32.1 seconds (base: 30s ±10%)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_pipeline_with_delay(stage_name, base_delay):\n",
    "    \"\"\"\n",
    "    Run a single pipeline with a randomized delay.\n",
    "\n",
    "    Args:\n",
    "        stage_name: Pipeline stage to run\n",
    "        base_delay: Base delay in seconds before starting\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (stage_name, success, rows_processed, error_msg)\n",
    "    \"\"\"\n",
    "    # Add random fuzzing: +/- 10% of base delay\n",
    "    fuzz_factor = random.uniform(0.9, 1.1)\n",
    "    actual_delay = base_delay * fuzz_factor\n",
    "\n",
    "    # Sleep with fuzzing to avoid simultaneous starts\n",
    "    time.sleep(actual_delay)\n",
    "\n",
    "    try:\n",
    "        query = create_pipeline(stage_name)\n",
    "\n",
    "        # Wait for completion\n",
    "        while query.isActive:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # Check result\n",
    "        if query.exception():\n",
    "            return (stage_name, False, 0, str(query.exception())[:50])\n",
    "        else:\n",
    "            last_progress = query.lastProgress\n",
    "            rows = last_progress.get(\"numInputRows\", 0) if last_progress else 0\n",
    "            return (stage_name, True, rows, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (stage_name, False, 0, str(e)[:50])\n",
    "\n",
    "\n",
    "def run_polling_pipelines():\n",
    "    \"\"\"\n",
    "    Main polling loop that runs all 5 pipelines in parallel every POLL_INTERVAL_SECONDS.\n",
    "    Includes fuzzing to avoid overloading.\n",
    "    \"\"\"\n",
    "    print(\"\uD83D\uDE80 STARTING AUTO LOADER POLLING (PARALLEL)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Track initial counts\n",
    "    initial_counts = {stage: get_table_count(TABLES[stage]) for stage in TABLES.keys()}\n",
    "    print(\"\\n\uD83D\uDCCA Initial row counts:\")\n",
    "    for stage, count in initial_counts.items():\n",
    "        print(f\"  {stage:12} : {count:,}\")\n",
    "\n",
    "    # Calculate end time\n",
    "    end_time = datetime.now() + timedelta(minutes=RUNTIME_MINUTES)\n",
    "    iteration = 0\n",
    "\n",
    "    # Main polling loop\n",
    "    while datetime.now() < end_time:\n",
    "        iteration += 1\n",
    "        remaining_minutes = int((end_time - datetime.now()).total_seconds() / 60)\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\n",
    "            f\"\uD83D\uDD04 Iteration #{iteration} - {datetime.now().strftime('%H:%M:%S')} ({remaining_minutes} min remaining)\"\n",
    "        )\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Run all 5 pipelines in parallel with staggered starts\n",
    "        print(\"  Starting pipelines in parallel with fuzzing...\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            # Submit all pipelines with small staggered delays\n",
    "            futures = []\n",
    "            for i, stage in enumerate(TABLES.keys()):\n",
    "                # Stagger starts by 0-2 seconds with fuzzing\n",
    "                base_delay = i * 0.5\n",
    "                future = executor.submit(run_pipeline_with_delay, stage, base_delay)\n",
    "                futures.append(future)\n",
    "                print(f\"    Submitted {stage:12} (delayed ~{base_delay:.1f}s ±10%)\")\n",
    "\n",
    "            # Wait for all to complete and collect results\n",
    "            print(\"\\n  Waiting for pipelines to complete...\")\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n  Pipeline Results:\")\n",
    "        for stage_name, success, rows, error in sorted(results, key=lambda x: x[0]):\n",
    "            if success:\n",
    "                print(f\"    {stage_name:12} : ✅ Processed {rows} rows\")\n",
    "            else:\n",
    "                print(f\"    {stage_name:12} : ❌ Failed: {error}\")\n",
    "\n",
    "        # Show current totals\n",
    "        print(\"\\n\uD83D\uDCCA Current row counts:\")\n",
    "        for stage in TABLES.keys():\n",
    "            current_count = get_table_count(TABLES[stage])\n",
    "            initial = initial_counts[stage]\n",
    "            new_rows = current_count - initial\n",
    "            print(f\"  {stage:12} : {current_count:,} total (+{new_rows:,} new)\")\n",
    "\n",
    "        # Wait for next iteration with fuzzing\n",
    "        if datetime.now() < end_time:\n",
    "            # Fuzz the wait time too: +/- 10%\n",
    "            fuzzed_wait = POLL_INTERVAL_SECONDS * random.uniform(0.9, 1.1)\n",
    "            print(\n",
    "                f\"\\n\uD83D\uDCA4 Waiting {fuzzed_wait:.1f} seconds (base: {POLL_INTERVAL_SECONDS}s ±10%)...\"\n",
    "            )\n",
    "            time.sleep(fuzzed_wait)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"✅ POLLING COMPLETE\")\n",
    "    print(\"\\n\uD83D\uDCCA Final Summary:\")\n",
    "    for stage in TABLES.keys():\n",
    "        final_count = get_table_count(TABLES[stage])\n",
    "        initial = initial_counts[stage]\n",
    "        total_new = final_count - initial\n",
    "        print(f\"  {stage:12} : {final_count:,} total (+{total_new:,} new rows)\")\n",
    "\n",
    "\n",
    "# Run the polling pipeline\n",
    "run_polling_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788d1b79-916e-4e80-86df-25d1a4be1ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quick Status Check\n",
    "\n",
    "Run this cell anytime to see current table counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0c5a25-b181-4ccf-a378-c421f4664e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Current Table Status\n========================================\ningestion    : 1,098,754 rows\nvalidated    : 356,942 rows\nanomalies    : 1 rows\nenriched     : 2,004 rows\naggregated   : 14,662 rows\n\n✅ No active streams (expected with availableNow trigger)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\uD83D\uDCCA Current Table Status\")\n",
    "print(\"=\" * 40)\n",
    "for stage in TABLES.keys():\n",
    "    count = get_table_count(TABLES[stage])\n",
    "    print(f\"{stage:12} : {count:,} rows\")\n",
    "\n",
    "# Check for active streams (should be none with availableNow trigger)\n",
    "active = spark.streams.active\n",
    "if active:\n",
    "    print(f\"\\n⚠️  {len(active)} active streams found (unexpected)\")\n",
    "else:\n",
    "    print(\"\\n✅ No active streams (expected with availableNow trigger)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "setup-and-run-autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}